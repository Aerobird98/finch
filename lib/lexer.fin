Token <- (||
  new: text type: type {
    (|Tokens| _text <- text, _type <- type )
  }
)

Tokens <- (||
  text { _text }
  type { _type }
)

Strings :: (
  alpha? {
    "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_" contains: self
  }

  operator? {
    "-+=/<>?~!,#$%^&*" contains: self
  }

  alpha-or-operator? {
    (self alpha?) or: { self operator? }
  }
)

Lexer <- (||
  new: source {
    (|Lexers|
      _source <- source
      _pos <- 0
      _start <- 0
    )
  }
)

Lexers <- (||
  each: block {
    token <- self next-token
    while: { token != nil } do: {
      block call: token
      token <-- self next-token
    }
  }

  advance {
    _pos <- _pos + 1
    _source at: _pos
  }

  done? { _pos >= _source length }

  advance-while: predicate {
    while: { self done? not } and: { predicate call: self current } do: {
      self advance
    }
  }

  current { _source at: _pos }

  next-token {
    self skip-whitespace

    if: _pos >= _source length then: { return nil }

    c <- self current
    if: c = "(" then: { return self single-token }
    if: c = ")" then: { return self single-token }
    if: c = "[" then: { return self single-token }
    if: c = "]" then: { return self single-token }
    if: c = "{" then: { return self single-token }
    if: c = "}" then: { return self single-token }
    if: c = "," then: { return self single-token }
    if: c = ";" then: { return self single-token }
    if: c = "." then: { return self single-token }
    if: c = "|" then: { return self single-token }
    if: (c alpha?) then: { return self read-name }

    c <- self advance
    "ERROR " + c
  }

  skip-whitespace {
    while: { self current = " " } do: {
      _pos <- _pos + 1
      _start <- _pos
    }
  }

  read-name {
    self advance-while: {|c| c alpha-or-operator? }
    self make-token
  }

  read-operator {
    ' When token types are implemented; needs to switch to name if it
    ' encounters a letter.
    self advance-while: {|c| c alpha-or-operator? }
    self make-token
  }

  single-token {
    self advance
    self make-token
  }

  make-token {
    result <- _source from: _start to: _pos
    _start <- _pos
    result
  }
)

' Tokenize some example strings.
tests <- [["()[]{}", "(", ")", "[", "]", "{", "}"]
          [";.|", ";", ".", "|"]
          ["foo bar a-+=/<>?~!,#$%^&*", "foo", "bar", "a-+=/<>?~!,#$%^&*"]
         ]

tests each: {|test|
  lexer <- Lexer new: (test at: 0)
  result <- []
  lexer each: {|token| result add: token }

  passed <- true
  from: 0 to: test length - 1 do: {|n|
    expected <- test at: n + 1
    actual <- result at: n
    if: expected != actual then: { passed <-- false }
  }

  if: passed then: {
    write-line: "PASS " + (test at: 0)
  } else: {
    write-line: "FAIL " + (test at: 0) + " != " + result
  }
}

' Try to tokenize some real files.
files <- [
  "../../lib/core.fin"
  "../../lib/lexer.fin"
  "../../test/arithmetic.fin"
  "../../test/arrays.fin"
  "../../test/booleans.fin"
  "../../test/cascade.fin"
  "../../test/fibers.fin"
  "../../test/literals.fin"
  "../../test/messages.fin"
  "../../test/objects.fin"
  "../../test/return.fin"
  "../../test/self.fin"
  "../../test/strings.fin"
  "../../test/switch.fin"
  "../../test/tco.fin"
  "../../test/test.fin"
  "../../test/variables.fin"
]

errors <- 0
files each: {|file|
  write-line: file
  source <- Io read-file: file
  lexer <- Lexer new: source
  lexer each: {|token|
    if: (token starts-with: "ERROR") then: { errors <-- errors + 1 }
  }
}

write-line: errors + " errors"
