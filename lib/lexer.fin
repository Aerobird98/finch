Token/None         <- ""
Token/LeftParen    <- "left paren"
Token/RightParen   <- "right paren"
Token/LeftBracket  <- "left bracket"
Token/RightBracket <- "right bracket"
Token/LeftBrace    <- "left brace"
Token/RightBrace   <- "right brace"
Token/Comma        <- "comma" // Includes newlines too.
Token/Semicolon    <- "semicolon"
Token/Dot          <- "dot"
Token/Pipe         <- "pipe"
Token/Arrow        <- "arrow"
Token/LongArrow    <- "long arrow"
Token/Bind         <- "bind"
Token/Self         <- "self"
Token/Undefined    <- "undefined"
Token/Break        <- "break"
Token/Return       <- "return"
Token/Number       <- "number"
Token/String       <- "string"
Token/Name         <- "name"
Token/Operator     <- "operator"
Token/Keyword      <- "keyword"
Token/IgnoreLine   <- "ignore line"
Token/Eof          <- "eof"
Token/Error        <- "error"

Token <- (||
  new: type text: text span: span {
    (|Tokens| _type <- type, _text <- text, _span <- span )
  }
)

Tokens <- (||
  type { _type }
  text { _text }
  span { _span }

  to-string { "'" + _text + "' (" + _type + ") " + _span }
)

SourceSpan <- (||
  new-file: file start: start end: end {
    (|self proto| _file <- file, _start <- start, _end <- end)
  }

  proto <- (||
    file { _file }
    start { _start }
    end { _end }

    to-string {
      pos <- _file index-to-position: _start
      _file path + ":" + pos line + "," + pos column }
  )
)

SourceFile <- (||
  new-path: path source: source {
    (|SourceFiles| _path <- path, _source <- source)
  }
)

SourceFiles <- (||
  path { _path }
  source { _source }

  // Given an index into the file, returns its line and column.
  // TODO(bob): This is quite slow.
  index-to-position: index {
    line <- 1
    column <- 1
    from: 0 to: index - 1 do: {|i|
      if: (_source at: i) = "\n" then: {
        line <-- line + 1
        column <-- 1
      } else: {
        column <-- column + 1
      }
    }

    (|| line <- line, column <- column )
  }
)

Strings :: (
  alpha? {
    "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_" contains: self
  }

  operator? {
    "-+=/<>?~!,#$%^&*" contains: self
  }

  identifier? {
    ((self alpha?) or: { self operator? }) or: { self digit? }
  }

  digit? {
    "0123456789" contains: self
  }

  whitespace? {
    " \t" contains: self
  }
)

Lexer <- (||
  new-path: path source: source {
    (|Lexers|
      _file <- SourceFile new-path: path source: source
      _source <- source
      _pos <- 0
      _start <- 0
      _eat-newlines? <- true
    )
  }
)

Lexers <- (||
  each: block {
    token <- self next-token
    while: { token != nil } do: {
      block call: token
      token <-- self next-token
    }
  }

  advance {
    _pos <- _pos + 1
    _source at: _pos - 1
  }

  eof? { _pos >= _source count }

  advance-while: predicate {
    while: { self eof? not } and: { predicate call: self current } do: {
      self advance
    }
  }

  current { _source at: _pos }
  next {
    if: _pos >= (_source count - 1) then: {
      ""
    } else: {
      _source at: _pos + 1
    }
  }

  // Reads the next token from the source and handles newlines.
  next-token {
    loop: {
      if: self eof? then: {
        span <- SourceSpan new-file: _file start: _pos end: _pos
        return Token new: Token/Eof text: "" span: span
      }

      token <- self next-token-raw

      done? <- true
      token type switch \
      case: Token/Comma do: {
        if: _eat-newlines? then: {
          // Discard newline.
          done? <-- false
        } else: {
          // Discard any newlines after this one.
          _eat-newlines? <- true
        }
      } ;
      case: Token/IgnoreLine do: {
        // Eat the ignore token.
        done? <-- false

        // And newlines after it.
        _eat-newlines? <- true
      } ;
      case: Token/Keyword     do: { _eat-newlines? <- true } ;
      case: Token/Operator    do: { _eat-newlines? <- true } ;
      case: Token/Pipe        do: { _eat-newlines? <- true } ;
      case: Token/Arrow       do: { _eat-newlines? <- true } ;
      case: Token/LongArrow   do: { _eat-newlines? <- true } ;
      case: Token/Semicolon   do: { _eat-newlines? <- true } ;
      case: Token/LeftParen   do: { _eat-newlines? <- true } ;
      case: Token/LeftBracket do: { _eat-newlines? <- true } ;
      case: Token/LeftBrace   do: { _eat-newlines? <- true } ;
      default: { _eat-newlines? <- false }

      if: done? then: { return token }
    }
  }

  // Reads the next token from the source. Doesn't do any newline normalization.
  next-token-raw {
    loop: {
      _start <- _pos
      c <- self current
      true switch \
      case: c = "("  do: { return self single-token: Token/LeftParen } ;
      case: c = ")"  do: { return self single-token: Token/RightParen } ;
      case: c = "["  do: { return self single-token: Token/LeftBracket } ;
      case: c = "]"  do: { return self single-token: Token/RightBracket } ;
      case: c = "{"  do: { return self single-token: Token/LeftBrace } ;
      case: c = "}"  do: { return self single-token: Token/RightBrace } ;
      case: c = ","  do: { return self single-token: Token/Comma } ;
      case: c = "\n" do: { return self single-token: Token/Comma } ;
      case: c = "\\" do: { return self single-token: Token/IgnoreLine } ;
      case: c = ";"  do: { return self single-token: Token/Semicolon } ;
      case: c = "."  do: { return self single-token: Token/Dot } ;
      case: c = "|"  do: { return self single-token: Token/Pipe } ;
      case: c = "/"  do: {
        self advance
        if: self current = "/" then: {
          // Line comment, so ignore the rest of the line and emit the line
          // token.
          self advance-while: {|c| c != "\n" }
          return self single-token: Token/Comma
        } else-if: self current = "*" then: {
          self skip-block-comment
        } else: {
          return self read-operator
        }
      } ;
      case: c = ":"  do: {
        self advance
        if: self current = ":" then: {
          self advance
          return self make-token: Token/Bind
        } else: {
          return self make-token: Token/Keyword
        }
      } ;
      case: c = "-" do: {
        self advance
        if: self current digit? then: {
          return self read-number
        } else: {
          return self read-operator
        }
      } ;
      case: c whitespace? do: { self advance-while: {|c| c whitespace? } } ;
      case: c alpha?      do: { return self read-name } ;
      case: c operator?   do: { return self read-operator } ;
      case: c = "\""      do: { return self read-string } ;
      case: c digit?      do: { return self read-number } ;
      default: {
        // If we got here, we failed to handle the current character.
        return self single-token: Token/Error
      }
    }
  }

  skip-block-comment {
    self advance
    self advance

    nesting <- 1

    while: { nesting > 0 } do: {
      true switch \
      case: self eof? do: {
        // TODO(bob): Unterminated comment, should return error.
        return
      } ;
      case: (self current = "/") && (self next = "*") do: {
        self advance
        self advance
        nesting <-- nesting + 1
      } ;
      case: (self current = "*") && (self next = "/") do: {
        self advance
        self advance
        nesting <-- nesting - 1
      } ;
      default: { self advance }
    }
  }

  read-name {
    self advance-while: {|c| c identifier? }

    // See if it ends with a ":"
    type <- Token/Name
    if: self current = ":" then: {
      self advance
      type <-- Token/Keyword
    }

    self make-token: type
  }

  read-operator {
    // When token types are implemented; needs to switch to name if it
    // encounters a letter.
    type <- Token/Operator

    while: { self eof? not } and: { self current identifier? } do: {
      if: self current alpha? then: { type <-- Token/Name }
      self advance
    }

    // See if it ends with a ":"
    if: self current = ":" then: {
      self advance
      type <-- Token/Keyword
    }

    self make-token: type
  }

  read-string {
    self advance

    text <- ""
    loop: {
      if: self eof? then: {
        return self make-token: Token/Error text: "Unterminated string"
      }

      if: self current = "\"" then: {
        self advance
        return self make-token: Token/String text: text
      }

      // Handle string escapes.
      if: self current = "\\" then: {
        self advance

        if: self eof? then: {
          return self make-token: Token/Error text:
              "Unterminated string escape."
        }

        c <- self advance
        c switch \
        case: "n"  do: { text <-- text + "\n" } ;
        case: "\"" do: { text <-- text + "\"" } ;
        case: "\\" do: { text <-- text + "\\" } ;
        case: "t"  do: { text <-- text + "\t" } ;
        default: {
          return self make-token: Token/Error text:
              "Unrecognized string escape '" + c + "'."
        }
      } else: {
        text <-- text + self current
        self advance
      }
    }
  }

  read-number {
    self advance-while: {|c| c digit? }
    self make-token: Token/Number
  }

  single-token: type {
    self advance
    self make-token: type
  }

  make-token: type {
    text <- (_source from: _start to: _pos)

    if: text = "<-"        then: { type <-- Token/Arrow }
    if: text = "<--"       then: { type <-- Token/LongArrow }
    if: text = "self"      then: { type <-- Token/Self }
    if: text = "undefined" then: { type <-- Token/Undefined }
    if: text = "break"     then: { type <-- Token/Break }
    if: text = "return"    then: { type <-- Token/Return }

    self make-token: type text: (_source from: _start to: _pos)
  }

  make-token: type text: text {
    span <- SourceSpan new-file: _file start: _start end: _pos
    Token new: type text: text span: span
  }
)

// Tokenize some real files.
files <- #[
  "../../lib/core.fin"
  "../../lib/lexer.fin"
  "../../test/arithmetic.fin"
  "../../test/arrays.fin"
  "../../test/booleans.fin"
  "../../test/cascade.fin"
  "../../test/fibers.fin"
  "../../test/literals.fin"
  "../../test/messages.fin"
  "../../test/objects.fin"
  "../../test/return.fin"
  "../../test/self.fin"
  "../../test/strings.fin"
  "../../test/switch.fin"
  "../../test/tco.fin"
  "../../test/test.fin"
  "../../test/variables.fin"
]

/*
tokens <- 0
errors <- 0
lines <- 0
files each: {|file|
  write-line: file
  source <- Io read-file: file
  lexer <- Lexer new-path: file source: source
  lexer each: {|token|
    tokens <-- tokens + 1
    if: (token type = Token/Error) then: {
      write-line: token
      errors <-- errors + 1
    }

    if: (token type = Token/Comma) then: { lines <-- lines + 1 }
  }
}

write-line: tokens + " tokens with " + errors + " errors in " + lines + " lines"
*/