var Token/None         = ""
var Token/LeftParen    = "left paren"
var Token/RightParen   = "right paren"
var Token/LeftBracket  = "left bracket"
var Token/RightBracket = "right bracket"
var Token/LeftBrace    = "left brace"
var Token/RightBrace   = "right brace"
var Token/Comma        = "comma" // Includes newlines too.
var Token/Semicolon    = "semicolon"
var Token/Dot          = "dot"
var Token/Pipe         = "pipe"
var Token/Arrow        = "arrow"
var Token/Equals       = "long arrow"
var Token/Bind         = "bind"
var Token/Self         = "self"
var Token/Undefined    = "undefined"
var Token/Break        = "break"
var Token/Return       = "return"
var Token/Number       = "number"
var Token/String       = "string"
var Token/Name         = "name"
var Token/Operator     = "operator"
var Token/Keyword      = "keyword"
var Token/IgnoreLine   = "ignore line"
var Token/Eof          = "eof"
var Token/Error        = "error"

var Token = [
  new: type text: text span: span {
    [|Tokens| _type = type, _text = text, _span = span ]
  }
]

var Tokens = [
  type { _type }
  text { _text }
  span { _span }

  to-string { "'" + _text + "' (" + _type + ") " + _span }
]

var SourceSpan = [
  new-file: file start: start end: end {
    [|self proto| _file = file, _start = start, _end = end ]
  }

  proto = [
    file { _file }
    start { _start }
    end { _end }

    to-string {
      var pos = _file index-to-position: _start
      _file path + ":" + pos line + "," + pos column }
  ]
]

var SourceFile = [
  new-path: path source: source {
    [|SourceFiles| _path = path, _source = source ]
  }
]

var SourceFiles = [
  path { _path }
  source { _source }

  // Given an index into the file, returns its line and column.
  // TODO(bob): This is quite slow.
  index-to-position: index {
    var line = 1
    var column = 1
    from (0) to (index - 1) do { i ->
      if (_source.at(i) == "\n") then {
        line = line + 1
        column = 1
      } else {
        column = column + 1
      }
    }

    [ line = line, column = column ]
  }
]

Strings :: (
  alpha? {
    "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_".contains(self)
  }

  operator? {
    "-+=/<>?~!,#$%^&*".contains(self)
  }

  identifier? {
    ((self alpha?).or { self operator? }).or { self digit? }
  }

  digit? {
    "0123456789".contains(self)
  }

  whitespace? {
    " \t".contains(self)
  }
)

var Lexer = [
  new-path: path source: source {
    [|Lexers|
      _file = SourceFile new-path: path source: source
      _source = source
      _pos = 0
      _start = 0
      _eat-newlines? = true
    ]
  }
]

var Lexers = [
  each.block {
    var token = self.next-token
    while { token != nil } do {
      block.call(token)
      token = self.next-token
    }
  }

  advance {
    _pos = _pos + 1
    _source.at(_pos - 1)
  }

  eof? { _pos >= _source.count }

  advance-while: predicate {
    while { self.eof?.not } and { predicate.call(self.current) } do {
      self.advance
    }
  }

  current { _source.at(_pos) }
  next {
    if (_pos >= (_source count - 1)) then {
      ""
    } else {
      _source.at(_pos + 1)
    }
  }

  // Reads the next token from the source and handles newlines.
  next-token {
    loop {
      if (self eof?) then {
        var span = SourceSpan new-file: _file start: _pos end: _pos
        return Token new: Token/Eof text: "" span: span
      }

      var token = self next-token-raw

      var done? = true
      token.type.switch \
      .case(Token/Comma) do {
        if (_eat-newlines?) then {
          // Discard newline.
          done? = false
        } else {
          // Discard any newlines after this one.
          _eat-newlines? = true
        }
      } ;
      .case(Token/IgnoreLine) do {
        // Eat the ignore token.
        done? = false

        // And newlines after it.
        _eat-newlines? = true
      } ;
      .case(Token/Keyword    ) do { _eat-newlines? = true } ;
      .case(Token/Operator   ) do { _eat-newlines? = true } ;
      .case(Token/Pipe       ) do { _eat-newlines? = true } ;
      .case(Token/Arrow      ) do { _eat-newlines? = true } ;
      .case(Token/LongArrow  ) do { _eat-newlines? = true } ;
      .case(Token/Semicolon  ) do { _eat-newlines? = true } ;
      .case(Token/LeftParen  ) do { _eat-newlines? = true } ;
      .case(Token/LeftBracket) do { _eat-newlines? = true } ;
      .case(Token/LeftBrace  ) do { _eat-newlines? = true } ;
      .default { _eat-newlines? = false }

      if (done?) then { return token }
    }
  }

  // Reads the next token from the source. Doesn't do any newline normalization.
  next-token-raw {
    loop {
      _start = _pos
      var c = self current
      true.switch \
      .case(c == "(" ) do { return self single-token: Token/LeftParen } ;
      .case(c == ")" ) do { return self single-token: Token/RightParen } ;
      .case(c == "[" ) do { return self single-token: Token/LeftBracket } ;
      .case(c == "]" ) do { return self single-token: Token/RightBracket } ;
      .case(c == "{" ) do { return self single-token: Token/LeftBrace } ;
      .case(c == "}" ) do { return self single-token: Token/RightBrace } ;
      .case(c == "," ) do { return self single-token: Token/Comma } ;
      .case(c == "\n") do { return self single-token: Token/Comma } ;
      .case(c == "\\") do { return self single-token: Token/IgnoreLine } ;
      .case(c == ";" ) do { return self single-token: Token/Semicolon } ;
      .case(c == "." ) do { return self single-token: Token/Dot } ;
      .case(c == "|" ) do { return self single-token: Token/Pipe } ;
      .case(c == "/" ) do {
        self advance
        if (self current == "/") then {
          // Line comment, so ignore the rest of the line and emit the line
          // token.
          self advance-while: { c -> c != "\n" }
          return self single-token: Token/Comma
        } else-if (self current == "*") then {
          self skip-block-comment
        } else {
          return self read-operator
        }
      } ;
      .case(c == ":") do {
        self advance
        if (self current == ":") then {
          self advance
          return self make-token: Token/Bind
        } else {
          return self make-token: Token/Keyword
        }
      } ;
      .case(c == "-") do {
        self advance
        if (self current digit?) then {
          return self read-number
        } else {
          return self read-operator
        }
      } ;
      .case(c whitespace?) do { self advance-while: { c -> c whitespace? } } ;
      .case(c alpha?     ) do { return self read-name } ;
      .case(c operator?  ) do { return self read-operator } ;
      .case(c == "\""    ) do { return self read-string } ;
      .case(c digit?     ) do { return self read-number } ;
      .default {
        // If we got here, we failed to handle the current character.
        return self single-token: Token/Error
      }
    }
  }

  skip-block-comment {
    self advance
    self advance

    var nesting = 1

    while { nesting > 0 } do {
      true.switch \
      .case(self eof?) do {
        // TODO(bob): Unterminated comment, should return error.
        return
      } ;
      .case((self current == "/") && (self next == "*")) do {
        self advance
        self advance
        nesting = nesting + 1
      } ;
      .case((self current == "*") && (self next == "/")) do {
        self advance
        self advance
        nesting = nesting - 1
      } ;
      .default { self advance }
    }
  }

  read-name {
    self advance-while: { c -> c identifier? }

    // See if it ends with a ":"
    var type = Token/Name
    if (self current == ":") then {
      self advance
      type = Token/Keyword
    }

    self make-token: type
  }

  read-operator {
    // When token types are implemented; needs to switch to name if it
    // encounters a letter.
    var type = Token/Operator

    while { self eof? not } and { self current identifier? } do {
      if (self current alpha?) then { type = Token/Name }
      self advance
    }

    // See if it ends with a ":"
    if (self current == ":") then {
      self advance
      type = Token/Keyword
    }

    self make-token: type
  }

  read-string {
    self advance

    var text = ""
    loop {
      if(self eof?) then {
        return self make-token: Token/Error text: "Unterminated string"
      }

      if(self current == "\"") then {
        self advance
        return self make-token: Token/String text: text
      }

      // Handle string escapes.
      if(self current == "\\") then {
        self advance

        if(self eof?) then {
          return self make-token: Token/Error text:
              "Unterminated string escape."
        }

        var c = self advance
        c.switch \
        .case("n" ) do { text = text + "\n" } ;
        .case("\"") do { text = text + "\"" } ;
        .case("\\") do { text = text + "\\" } ;
        .case("t" ) do { text = text + "\t" } ;
        .default {
          return self make-token: Token/Error text:
              "Unrecognized string escape '" + c + "'."
        }
      } else {
        text = text + self current
        self advance
      }
    }
  }

  read-number {
    self advance-while: { c -> c digit? }
    self make-token: Token/Number
  }

  single-token: type {
    self advance
    self make-token: type
  }

  make-token: type {
    var text = _source.from(_start) to(_pos)

    if (text == "="        ) then { type = Token/Equals }
    if (text == "self"     ) then { type = Token/Self }
    if (text == "undefined") then { type = Token/Undefined }
    if (text == "break"    ) then { type = Token/Break }
    if (text == "return"   ) then { type = Token/Return }

    self make-token: type text: _source.from(_start) to(_pos)
  }

  make-token: type text: text {
    var span = SourceSpan new-file: _file start: _start end: _pos
    Token new: type text: text span: span
  }
]

// Tokenize some real files.
var files = [
  "../../lib/core.fin"
  "../../lib/lexer.fin"
  "../../test/arithmetic.fin"
  "../../test/arrays.fin"
  "../../test/booleans.fin"
  "../../test/cascade.fin"
  "../../test/fibers.fin"
  "../../test/literals.fin"
  "../../test/messages.fin"
  "../../test/objects.fin"
  "../../test/return.fin"
  "../../test/self.fin"
  "../../test/strings.fin"
  "../../test/switch.fin"
  "../../test/tco.fin"
  "../../test/test.fin"
  "../../test/variables.fin"
]

var tokens = 0
var errors = 0
var lines = 0
files.each { file ->
  write-line(file)
  var source = Io.read-file(file)
  var lexer = Lexer new-path: file source: source
  lexer.each { token ->
    tokens = tokens + 1
    if (token type == Token/Error) then {
      write-line(token)
      errors = errors + 1
    }

    if (token type == Token/Comma) then { lines = lines + 1 }
  }
}

write-line(tokens + " tokens with " + errors + " errors in " + lines + " lines")
