Token/None         <- ""
Token/LeftParen    <- "("
Token/RightParen   <- ")"
Token/LeftBracket  <- "["
Token/RightBracket <- "]"
Token/LeftBrace    <- "{"
Token/RightBrace   <- "}"
Token/Comma        <- "," ' Includes newlines too.
Token/Semicolon    <- ";"
Token/Dot          <- "."
Token/Pipe         <- "|"
Token/Arrow        <- "<-"
Token/LongArrow    <- "<--"
Token/Bind         <- "::"
Token/Self         <- "self"
Token/Undefined    <- "undefined"
Token/Break        <- "break"
Token/Return       <- "return"
Token/Number       <- "number"
Token/String       <- "string"
Token/Name         <- "name"
Token/Operator     <- "operator"
Token/Keyword      <- "keyword"
Token/IgnoreLine   <- "ignore line"
Token/Eof          <- "eof"
Token/Error        <- "error"

Token <- (||
  new: type text: text span: span {
    (|Tokens| _type <- type, _text <- text, _span <- span )
  }
)

Tokens <- (||
  type { _type }
  text { _text }
  span { _span }

  to-string { "'" + _text + "' (" + _type + ") " + _span }
)

SourceSpan <- (||
  new-file: file start: start end: end {
    (|SourceSpans| _file <- file, _start <- start, _end <- end)
  }
)

SourceSpans <- (||
  file { _file }
  start { _start }
  end { _end }

  to-string { _file path + ":" + _start + ":" + _end }
)

SourceFile <- (||
  new-path: path source: source {
    (|SourceFiles| _path <- path, _source <- source)
  }
)

SourceFiles <- (||
  path { _path }
  source { _source }
)

Strings :: (
  alpha? {
    "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_" contains: self
  }

  operator? {
    "-+=/<>?~!,#$%^&*" contains: self
  }

  alpha-or-operator? {
    (self alpha?) or: { self operator? }
  }
)

Lexer <- (||
  new-path: path source: source {
    (|Lexers|
      _file <- SourceFile new-path: path source: source
      _source <- source
      _pos <- 0
      _start <- 0
    )
  }
)

Lexers <- (||
  each: block {
    token <- self next-token
    while: { token != nil } do: {
      block call: token
      token <-- self next-token
    }
  }

  advance {
    _pos <- _pos + 1
    _source at: _pos
  }

  done? { _pos >= _source length }

  advance-while: predicate {
    while: { self done? not } and: { predicate call: self current } do: {
      self advance
    }
  }

  current { _source at: _pos }

  next-token {
    self skip-whitespace

    if: _pos >= _source length then: { return nil }

    c <- self current
    if: c = "(" then: { return self single-token: Token/LeftParen }
    if: c = ")" then: { return self single-token: Token/RightParen }
    if: c = "[" then: { return self single-token: Token/LeftBracket }
    if: c = "]" then: { return self single-token: Token/RightBracket }
    if: c = "{" then: { return self single-token: Token/LeftBrace }
    if: c = "}" then: { return self single-token: Token/RightBrace }
    if: c = "," then: { return self single-token: Token/Comma }
    if: c = ";" then: { return self single-token: Token/Semicolon }
    if: c = "." then: { return self single-token: Token/Dot }
    if: c = "|" then: { return self single-token: Token/Pipe }
    if: (c alpha?) then: { return self read-name }

    ' If we got here, we failed to handle the current character.
    self single-token: Token/Error
  }

  skip-whitespace {
    while: { self current = " " } do: {
      _pos <- _pos + 1
      _start <- _pos
    }
  }

  read-name {
    self advance-while: {|c| c alpha-or-operator? }
    self make-token: Token/Name
  }

  read-operator {
    ' When token types are implemented; needs to switch to name if it
    ' encounters a letter.
    self advance-while: {|c| c alpha-or-operator? }
    self make-token: Token/Operator
  }

  single-token: type {
    self advance
    self make-token: type
  }

  make-token: type {
    span <- SourceSpan new-file: _file start: _start end: _pos
    text <- _source from: _start to: _pos
    _start <- _pos
    Token new: type text: text span: span
  }
)

' Tokenize some example strings.
tests <- [["()[]{}", "(", ")", "[", "]", "{", "}"]
          [";.|", ";", ".", "|"]
          ["foo bar a-+=/<>?~!,#$%^&*", "foo", "bar", "a-+=/<>?~!,#$%^&*"]
         ]

tests each: {|test|
  lexer <- Lexer new-path: "(test)" source: (test at: 0)
  result <- []
  lexer each: {|token| result add: token }

  passed <- true
  from: 0 to: test length - 1 do: {|n|
    expected <- test at: n + 1
    actual <- result at: n
    if: expected != actual then: { passed <-- false }
  }

  if: passed then: {
    write-line: "PASS " + (test at: 0)
  } else: {
    write-line: "FAIL " + (test at: 0) + " != " + result
  }
}

' Try to tokenize some real files.
files <- [
  "../../lib/core.fin"
  "../../lib/lexer.fin"
  "../../test/arithmetic.fin"
  "../../test/arrays.fin"
  "../../test/booleans.fin"
  "../../test/cascade.fin"
  "../../test/fibers.fin"
  "../../test/literals.fin"
  "../../test/messages.fin"
  "../../test/objects.fin"
  "../../test/return.fin"
  "../../test/self.fin"
  "../../test/strings.fin"
  "../../test/switch.fin"
  "../../test/tco.fin"
  "../../test/test.fin"
  "../../test/variables.fin"
]

errors <- 0
files each: {|file|
  write-line: file
  source <- Io read-file: file
  lexer <- Lexer new-path: file source: source
  lexer each: {|token|
    write-line: token
    if: (token type = Token/Error) then: {
      errors <-- errors + 1
    }
  }
}

write-line: errors + " errors"
